i shal be up before you are awake, i shall be at field before you are up.

rsync -avihP -e "ssh -p 1422" 128.199.162.222:/home/bhavesh/vagrant vagrant

rsync -avihP --progress -e "ssh -p 1422" vishnu.byteoflinux.com:/home/bhavesh/AWS_SYSOP .

###Docker commands####
usermod -aG docker bhavesh

docker pull images --- pull images from the repos

docker pull ubuntu -- pulls ubuntu specific images

docker run -t -i imageid /bin/bash --- will get you a bash shell from the image

dcoker ps		--- shows docker process

docker ps -a 		--- shows all of the docker process

docker run it --rm ubuntu /bin/bash	this will remove the image on exit
docker run -it --rm -v /home/bhavesh:/mydocker ubuntu /bin/bash
docker run -it --rm -v /home/bhavesh:/mydocker -u 1000:1000 ubuntu /bin/bash

docker run -t -i --name bhavesh1 -v /root/mydocker:/mydock ubuntu /bin/bash
The above command will create a instance with name bhavesh with mount /root/mydocker directory and it will mount it as mydocker on the instance... this image will be created using the ubuntu image.

docker run -t -i --name bhavesh2 --volumes-from bhavesh1 ubuntu /bin/bash

The above command will create bhavesh2 instance with all th volumes from bhavesh1


docker run -it --rm -v $(pwd):/var/www/html apache2ctl -D FOREGROUND
The above command will start apache with content of my pwd in /var/www/html/

docker ps --no-trunc=true

docker commit -a "bhavesh" -m "message" imageid newimagename

docker run -d  --name="imagename" -p=80:80 imagename --- to expose a port from the docker container


What is java class file

The Java class file is a precisely defined format for compiled Java. Java source code is compiled into class files that can be loaded and executed by any JVM

What is jvm

Java virtual machine (JVM), an implementation of the Java Virtual Machine Specification, interprets compiled Java binary code (called bytecode) for a computer's processor (or "hardware platform") so that it can perform a Java program's instructions.

ps -auxf | grep  '/usr/sbin/httpd' | grep "^root" | awk '{ print $2 }'

1012,1032s/^/#/g ###################### Multiple line together in vim


Creating New certificate using openssl

openssl req -new -x509 -nodes -out server.crt -keyout  server.key
					#			#
					#			#
					#			#

EOF				     Publickey		   Privatekey		
############

A Linux machine with ext2 filesystem if it crashed then at reboot, kernel would have to scan the entire filesystem for corrupt files. EXT3 has something called as journalingm what it does is this, When a kernel open a file for any operation it marks the file as open in the filesystem so at the time of system crash it just has to check the authencity of those particular files.

EXT4 has option of ecnrypting the filesystem at the time of installation.

EOF 
############
APache and tomcat integration using proxypass

Below line is to be added in apache configuration
ProxyPass / "http://tomcat.technotronix.com:8080/"
ProxyPassReverse / "http://tomcat.technotronix.com:8080/"

EOF
#########################
Apache website clustering using proxypass Load balancer

<Proxy balancer://mycluster>
BalancerMember http://ob.technotronix.com/openbravo
BalancerMember http://ob1.technotronix.com/openbravo
</Proxy>
ProxyPass /test "balancer://mycluster/"

#########################
Virtual hosts in tomcat (Serving mutliple webapps wit different hostname in tomcat )

Tomcat server.xml layout
<server>
	<Service name="Catalina">
		<Engine name="Catalina">
			<Host name="localhost">		-----> we are concern about this
			</Host>
		</Engine>
	</Service>
</server>
		
You have toedit this line rarther add mor line like this

<Host name="www.bhavesh.com"  appBase="bhavesh_webapps" -----> Webapps is the direcotry
            unpackWARs="true" autoDeploy="true"/>		where the context of this 
<Host name="www.patel.com"  appBase="patel_webapps"		virtual host is stored
            unpackWARs="true" autoDeploy="true"/>
<Host name="www.bkp.com"  appBase="bkp_webapps"
            unpackWARs="true" autoDeploy="true"/>


#########################
Difference between mod_jk and proxypass

mod_proxy
:Pros
      o No need for a separate module compilation and maintenance. mod_proxy,
        mod_proxy_http, mod_proxy_ajp and mod_proxy_balancer comes as part of 
        standard Apache 2.2+ distribution
      o Ability to use http https or AJP protocols, even within the same 
        balancer.
* Cons:
      o mod_proxy_ajp does not support large 8K+ packet sizes.
      o Basic load balancer
      o Does not support Domain model clustering

mod_jk

* Pros:
      o Advanced load balancer
      o Advanced node failure detection
      o Support for large AJP packet sizes
* Cons:
      o Need to build and maintain a separate module
########################


#########################
Apache and tomcat integration using mod_jk


1) Install apache and make sure mod_rewrite and mod_proxy modules are available to apache

2) Download and Install the mod_jk connector
	untar the source code of tomcat-connectors-1.2.41-src.tar.gz
3) cd tomcat-connectors-1.2.41-src/native
	
4) start the mod_jk compilation process Incase apxs is not installed you need to install it .

* apxs is a tool for building and installing extension modules for the Apache HyperText Transfer Protocol (HTTP) server. This is achieved by building a dynamic shared object (DSO) from one or more source or object files which then can be loaded into the Apache server under runtime via the LoadModule directive from mod_so.
  Install apache2-dev for debian based os and httpd-devel for rhel based os. Once apxs is successfully install, we can continue with mod_jk compilation.

	cd /tomcat-connectors-1.2.41-src/native

	./configure --with-apxs=/usr/sbin/apxs  		Incase if gcc is not install it will throw errors, make sure it is installed.
	sudo make
	sudo make install
	
	
5) Make sure the module has been installed by checking into /usr/lib64/httpd/modules, But you still need to mention about the module in apache conf file.
	LoadModule jk_module modules/mod_jk.so

6) Configuring JK Connector:-
	
	a)Create a worker.properties in /etc/httpd/conf/ with following content

		worker.list=tomcat,tomcat2
		worker.tomcat.type=ajp13
		worker.tomcat.host=192.168.122.30
		worker.tomcat.port=8009
	 
       	  	worker.tomcat2.type=ajp13
                worker.tomcat2.host=192.168.122.31
                worker.tomcat2.port=8009
	
	b)Make entry in httpd.conf
	
		JkWorkersFile 	"conf/worker.properties"
		JkLogfile	"logs/jk.log"
		
		JkMount /mytomcat*              tomcat2
		JkMount /sample*                tomcat

Now it should work fine webserver/mytomcat ------------> will open tomcat2
			webserver/sample   ------------> will open tomcat	

7) Incase if you want to go further with the configuration and create virtual hosts for the same.
	
	Create virtual host for the above config in apache
	
	NameVirtualHost *:80
	<VirtualHost *:80>
	ServerName mytomcat.technotronix.com
	JkMount /mytomcat*	tomcat2
	</VirtualHost>
	
	<VirtualHost *:80>
	ServerName sample.technotronix.com
	JkMount /sample*	tomcat2
	</VirtualHost>

Make sure that mytomcat.technotronix.com and sample.technotronix.com are pointing to your webserver in dns.

EVEN WITH THE ABOVE CONFIGURATION FOR VIRTUAL HOST, THINGS WON'T WORK AS WE NEED TO MAKE USE OF mod_rewrite module for url base regular expression catching.

8) Proper virtual host config

# Tomcat integration virtual host
 NameVirtualHost *:80
         
       <VirtualHost *:80>
                        ServerName mytomcat.technotronix.com
                        RewriteEngine on
                        RewriteLog logs/apache-mod_rewite
                        RewriteRule ^/(.*)$ /mytomcat/$1 [l,PT]
                JkMount /*      tomcat2
        </VirtualHost>
         
        <VirtualHost *:80>
                         ServerName sample.technotronix.com
                         RewriteEngine on
                         RewriteLog logs/apache-mod_rewite
                         RewriteRule ^/(.*)$ /sample/$1 [l,PT]
                 JkMount /*        tomcat
         </VirtualHost>

EOF
###########
Tomcat load balance using mod_jk

1) We create worker.properties file with the following content

	worker.list=balancer,stat

	worker.tomcat.type=ajp13
	worker.tomcat.host=192.168.122.30
	worker.tomcat.port=8009
	worker.tomcat.lbfactor=10

	worker.tomcat2.type=ajp13
	worker.tomcat2.host=192.168.122.31
	worker.tomcat2.port=8009
	worker.tomcat2.lbfactor=10
	
	worker.balancer(canbeanyname).type=lb
	worker.balancer.balance_workers=tomcat,tomcat2
	worker.stat.type=status

Change in httpd.conf 		JkMount /* 	balancer
				JkMount /status stat

EOF
########################

Tomcat based session affinity load balancing:-

Need to add jvmroute=workernamegiveninworker.properties under engine tag in server.xml in each worker's server.xml file according to the name given in worker.properties.

by default its like this <Engine name="Catalina" defaultHost="localhost">

According to my configuration what i will metion is below.

tomcat instance server.xml	<Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat">
tomcat2 instance server.xml	<Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat2">

* BUT with this moodle of loadbalancing another issue arrises if a user has been using a instance and all his work has been given to one of the tomcat instance incase of failure of that tomcat instance, the data specific to the user connected to that instance will get lost. In this the user will be given a new session which is from another tomcat instance.

EOF
########################

Activating tomcat manager role.
In tomcat-user.xml file unxomment the user block and create a new user
<user username="bhavesh" password="admin123" roles="manager-gui"/>

EOF
###########
APACHE installation form source code:- 30-09-2015


CASE 1 Fully dynamic webserver
******
./configure --with-apr=/usr/local/apr-httpd/ --with-apr-util=/usr/local/apr-util-httpd/ --prefix=/usr/local/apache2d/ --enable-mods-shared=all

aprutil
./configure --prefix=/usr/local/apr-util-httpd --with-apr=/usr/local/apr-httpd/

apr-httpd
./configure  --prefix=/usr/local/apr-httpd/

make & make install 

This will create a fully dynamic webserver, only the required modules are in core and other compiled as shared.


CASE2
******
Fully dynamic webserver with mod_userdir builtin staticlly


./configure --with-apr=/usr/local/httpd/apr-httpd --with-apr-util=/usr/local/apr-util-httpd/ --prefix=/usr/local/apache2d/ --enable-mods-shared=all -enable-userdir=static


CASE3
******
Fully dynamic webser with mod_userdir disabled

./configure --with-apr=/usr/local/httpd/apr-httpd --with-apr-util=/usr/local/apr-util-httpd --prefix=/usr/local/apache2d --enable-mod-shared=all --disable-userdir


CASE4
******
Fully dynamic server webserver with all the modules being staticly compiled

./configure --with-apr=/usr/local/apr-httpd --with-apr-util=/usr/local/apr-util-httpd --prefix=/usr/local/apache2/ --enable-mods-static=all
#####################################################################################
Url to file system Mapping:-
1. Publish content form various locations in and outside if the filesystem.
2.

ALias | ScriptAlias (CGI)

3. USe AliasMatch with Regular expression
	
	AliasMatch ^/docs/(.*pdf) /var/www/docs2/$1


#####################################################################################

Redirection
Features:
1) Sends Client to new destination
2) Update Browser || Http caller url
3) Remaps: URL-Path beginning with '/' ton new url (scheme and hostname | /path)
4) Implementation of: 'mod_alias' as Redirect,RedirectMatch, RedirectPermanent,
 RedirectTemp
5) Take precedence over: Alias bases directives regardless configuration 
6) Default Status = 302 (Temporary ) if unspecified
7) Supports codes in the range of 300-399 (standard redirection codes) && 410
8) common Codes:-
 a) 301 = pemanent
 b) 302 = temporary
9)Usage: Redirect [Match|Permanent|Temp] [status] URL=path URL

Redirect 301 /docs http://192.168.122.121/docs2
	  \
	   \
           Code for Permanent redirect

RedirectMatch ----supports Regexs

RedirectMatch (.*)\.doc $1.pdf		-- applies .pdf substitution across the site
RedirectMatch /docs/(.*)\.doc  /PDF/$1.pdf	places client into '/PDF' directory

##
Htaccess
Features:
1. Indirect site control via: FTP, SFTP, etc- via clients i.e Dreamweaver, Filezilla, etc.
2. On-demand, configuration updates toyour site managed by apache
3. Useful in the event that you lack 'root' access to manipulate the server;s configuration
4. Substitute for: <Directory></Directory> Directives
Note: We need not include <Directory. tags because '.htaccess' files are per directory
5. Recursive control of direcoty branches
6. Per-directory granular control
7. Caveat: Performance impacts as Apache must read each .htaccess file per directory
 per request
8. Choice of file name - Driven by 'AccessFilName' directive
9. Permitted Directives are restricted by:'AllowOverride' directive --specified in server's main configuration.
10. Ideal for shared webserver environment

Every request via .htaccess genrate disk io
whereas, <Directory> blocks are cached

##
Content negotiation
 




########################################## APACHE MPMS ######################################










##############


Server Hardening tips

Disable  PermitRootLogin,AllowTCpForwarding,x11Forwarding in sshd config


EOF
###########

LEARNING GIT	20-09-2015


git init 			---- to initialize git 

git status 			---- shows git status

git add filename 		---- will add it to the repository

git commit 			--- commiting the change

git commit -m "message"		-commit without going into the editor

git config --global user.name "bhaveshpatel"
git config --global user.email "bhaveshpatel826@gmail.com"
git config --global color.ui true

git log --graph --decorate --oneline --all --date-order
git config --global merge.conflictstyle diff3
git config --global core.editor vim
git config --global --replace-all alias.l "log --graph --decorate --oneline -all --date-order"

git diff 		--- to check diff in between the stage that is once you do git add after change the file and agian you change it but you don't add itm it will show you the difference

git diff --staged

git diff HEAD

git diff --cached

git diff 1e13aa1 870d54b 	---Testing the diff between the commits

Branches are kind of pointer to the commit

git branch

git branch branchname

git checkout branchname		to checkout a branch and you switch to the given branch name

git stash

git checkout master
git merge feature	we merged feature into master, but you need to switch to master branch its like joing feature to master, so you need to be in master first and then merge feature to it.

git push -u origin master

git branch test			--would create a test branch
 
Incase you want to ingnore some files from the git directory that you need to create the .gitignore file inside that dir.

https://github.com/github/gitignore

Files in the staging area cannot be deleted unless you do a force remove.
			touch deleteme.txt
			git add deleteme.txt
			git rm deleteme.txt
			Its not gonna allow you to delet the deleteme.txt file
			error:'deleteme.txt' has changes staged in the index
	Then you have to forcefully delete it i.e	git rm -f deleteme.txt

git log
git log --pretty=oneline

git log --pretty=format:"%h : %an : %ar : %s"
			%h 	shows the hash
			%an	show who created the file
			%ar	date it was changed
			%s	show the first line of the file

git log -p -2			show the log for last 2 commit
git log --stat			shows abbreviated log
git log --since=1.weeks		shows log since last week
git log --since="2015-09-21"	shows log since the mentioned date
git log --befor="any date"	shows log before the mentioned date

git commit --amend		allows you to change the previously made commit

Incase if you decide to stage a file and you do the command git add filename and later you realise that you dont want to stage it you issue this command to over come this:-	git reset HEAD filename

git commit -m "commit message format"
Best parctice to follow the below mentioned format
	1)exactly the problem which also acts as a heading
	2)In a paragraph format describe the original problem that was addressed.
	3)Then go on to describe the result of the change.
	4)Any future improvements


git remote -v 			--show remote repos name
git pull remoterepoaddress	

git remote rename origin sf		--- this will rename the git remote directory from origin to sf

Creating a tag for my previous commit, commit that i just made.
git tag -a 0.2 -m "Version 0.2"

 Now say that we have a previous commit how are we going to add a tag to those commits
	1)List out all of your commits on to the screen 		git log --pretty=oneline
	2)Copy the hash (just few charcters) foreg:-
		
	Below are my commits,

	bhavesh@workstation:~/mydocs$ git log --pretty=oneline
	46c9c20323e163f44a07cc629bdb47b59ab718e3 dailycommit
	983455e4fcfd3a857201d2a0c250cf427bb4301f aftersomeadded git command
	7674f67b2819538e46a5038490c69e7d040c140d antohertestcommitonbitbucket
	eecae0df034ef58ff0547d669b03d3c411d9095f mypersonaldoc
	
	3) Now we want to tag  7674f67b2819538e46a5038490c69e7d040c140d antohertestcommitonbitbucket
	4) git tag -a v0.5 7674f67b28

You can also push the tags to the remote repo

git push reponame v0.5
git push reponame --tags		That will add all the tags to the repo

Creating aliases for Git commands.

git config --global alias.co commit		---This creates alias "co" for "commit"

How to clone a git repository

git clone remoterepoaddress
for eg:-
git clone https://github.com/bhaveshpatel826/Python.git

Branching In git means taking the project in your own direction without affecting the the master code. So that you can go an work with the code without potentially introducing unstable code to the master branch.
 

######################################################## LEARNING NAGIOS 09-10-2-15################################################################

Defaults checks are perfomed every 5 minutes

Checks:-
Active (Default, Nagios-Initiated) Passive (externally-initiated) checks are supported

Nagios allows us to monitor OBJECTs

objects:-
a:- Host --- physical and or virtual
b:- Services --- http,smpt,dns,etc
c:- Contacts --- Defined for notification purposes
d:- Commands --- eg:- command that changes dns entry when a host is down
e:- Timeperiod-- When a particulat host should be checked, when a particular host is
		 notified

Supports notifications:-
a) SMS
b) E-mail
c) Custom -- ie write to a file, log to db, log to file for syslog server

Notification Schedules (notification_period)
Notification States (notification_options)
Paralled monitoring (service | host checks) : i.e Nmap|Nessus
Extensible via plugins eg nrpe (nagios remote plugin executor)

Has split (Distributed) Configuration files
	Main (Nagios.cfg) ---Nagios Daemon (core)
	Objects -host|Hostgropups|Services | Contacts | Contact Group | commands ,etc
	Resources -Data that should not be ready by the cgi 
	cgi webinterface


Nagios INSTALLATION & CONFIGURATION:-

1)Configures required items, including Apache HTTPD
2)Auto-monitoring of localhost

Note:-Postfix or similar mta is required

nagios.cfg primary config file for nagios Core
	cfg_file -specifies config file to include i.e hosts, serviecs, contacts, etc.
	cfg_dir	 -specifies directory to include, containing config files (*.cfg) to
		  process.


/usr/lib/nagios/plugins each command are systm binaries that reside in this directory.

Starting with the monitoring:--

By default nagios monitors itself.

ICMP(PING) Moniotring
1) Ping Monitoring using /usr/lib/nagios/plugins/check_host  --> check_icmp

This basic check_assumes ICMP PING (echo|request|reply) are supported by network

If monitoring host reside on fpregin host then ensure ping capabilites at l3 level(router,switch)

2) Predefined template ro handle most monitors
 a)'generic-host' -checks target using ICMP with sensible defaults
 b)'generic-host' - can be inherited by the host definition using 'use generic-host'

3) Host and various object are read from '/etc/nagios3/conf.d'
 a) default localhost.cfg as a template can be used

NOTE:-CGI and NAgios do not update hostlist on the fly

4) check_interval to the monitoring host config in conf.d directory
	check_interval 2

5) hosts are maintained on a rolling basis and may not necessarily be checked simultaneously

TCP| UDP Monitoring
1) Directly monitors publicly accessible services published
2) Check commands 'check_*' can be run manually from shell to confirm

Many services i.e mysql will run with unix daemon sockets excluslively and that this configuratio prohibits tcp|UDP access

Monitoring services based on tcp services:-
example for ssh and http service

# check that web services are running
define service {
        hostgroup_name                  http-servers
        service_description             HTTP
        check_command                   check_http
        use                             generic-service
        notification_interval           0 ; set > 0 if you want to be renotified
}

# check that ssh services are running
define service {
        hostgroup_name                  ssh-servers
        service_description             SSH
        check_command                   check_ssh
        use                             generic-service
        notification_interval           0 ; set > 0 if you want to be renotified
}

Nagios Remote Process Executor (NRPE)
Features:
 1) Ascertain local data:
  a) CPU Load
  b) RAM
  c) Current Users
  d) Disk Usage
 2) Supports
  a) Direct (runs loacl checkks via NRPE tunnel)
  b) Indirect (runs common checks via NRPE tunnel) 
eg:- If nagios monitor instance does'not have access to target services NRPE
	(Indirect) mode works well.
 3)Restrcited set of command may run on the target: '/etc/nagios/nrpe.cfg'
 4)Ability to execute remote plugins
 5)Supports SSL connections (DEFAULT)
 6)Supports Clear-Text using: '-n' options with:'check_nrpe'
 7)Uses TCP:5666 - Open on firewall
 8)Implemented ass 2 components:
  a) Client - Runs on monitoring server
  b) Server - Runs on the target (Monitored) Server

Tasks:-
1. Install NRPE Server - Target Server - Runs NRPE Daemon - Requires  'nagios-plugins' 	  package
 a) Install nagios-nrpe-server
 b) /etc/nagios/nrpe.cfg update allowed_hosts to permit nagios monitoring server access
2. Install NRPE - CLient  - Monitoring Server
 a) apt-get install nagios-nrpe-plugin
	/etc/nagios-plugins/config/check_nrpe.cfg
	/usr/lib/nagios/plugins/check_nrpe
3. Run Checks Manually From Nagios Server to Confirm Functionality.
Note:- Default communication uses SSL

eg:-
/usr/lib/nagios/plugins/check_nrpe -H 192.168.122.70 -c check_vda1
/usr/lib/nagios/plugins/check_nrpe -H 192.168.122.70 -c check_total_procs

Object Groups:-
Features
 1) Grouping of commonly classed objects: hosts, services, contacts, etc.
 2) Visual representation in CGI
 3) Configuration ingeritance: ie Services mapped to HostGroups
 4) Defaults are /etc/nagios3/conf.d/hostgroups.cfg

Task:-
 1) Extend the default admins to include other contacts
 2) Add another block of 

	define contact{
        contact_name                    bhavesh
        alias                           linuxadmin
        service_notification_period     24x7
        host_notification_period        24x7
        service_notification_options    w,u,c,r
        host_notification_options       d,r
        service_notification_commands   notify-service-by-email
        host_notification_commands      notify-host-by-email
        email                           bhaveshpatel826@gmail.com

        }

	The above blocke can be edited as per our requirement.



Make sure you make the new contact the member of the contact gorup.

define contactgroup{
        contactgroup_name       admins
        alias                   Nagios Administrators
        members                 root,bhavesh

By creating another bloxk of the same code you can create another contact group

define contactgroup{
         contactgroup_name       Helpdesk
         alias                   Nagios Administrators
         members                 root,bhavesh

###

service_notification_period can be edited from the timperiodconfig file

###

Create hostgorups, add members to it and then add these hostgorups to the 
services groups, so all the members of the hostgourps will be acquire these services



###################################################### Learning Iptables 23-09-2015 ################################################################

Iptables is an front end to manage Netfilter which is integrated with kernel
Iptables function primarily at  osi layer 3(Network) and 4(Transport)
Iptables can manage icmp protocol which tends to operate at 3 and 4 layer of osi
Iptables comes along with various modules which increase itse functionality, and which can be loaded dynamically on the fly.

There are 3 default tables which cannot be deleted
	a) mangle -- alter packets withing tcp/icmp/udp/etc

	b) NAT	  -- to change source/dest to another souce/dest
 
	c) Filter -- Ip packet filtering (INPUT,FORWARD,OUTPUT)

iptables consist of tables within the table there are chains and with the chainsther are rules


*ACL SYTNAX

a)Make use of iptables command for constructing iptables based rules
	b)itpables  commands
	1)name of the chain - action what to do ( Append/Insert/Replace)
	2) name of the table --magle/nat/userdefined
	3)(filter),src/dest address)
	4)dest ports/src ports (optionally) -p --sport/--dport
	5)Jump/Target  -j ACCEPT/DROP/DENY/REJECT/LOG

for eg:- BLock source ip from communicating to our system
	iptables -A INPUT -s 192.168.55.1 -j DROP
iptables  append to input chain source 192.168.55.1 jump target drop

* Saving and Restoring Rules via text files
a)iptables-save 	Dumps rules on the screen you need to redirect it.
b)iptables-restore	You can restore for a file

iptables -F  	Flush all the rules in all the chain in filter table.

c) To restore rules use iptables-restore < firewallrules.txt
d) To save rules use iptables-save > firewallrules.txt

Chain Mgmt In varios tables (Mangle/filter/nat/user-defined)

iptables -L 	will list the filter tables showing three chains
iptables -L -t nat this will list nat table
iptables -L -t mangle
iptables -L -v 		shows the number of packets have passed
iptables -L -v -n --line-numbers shows numbers for the rules

for eg:-
permit ssh
iptables -A INPUT -p tcp --dport 22 -j ACCEPT

deny telnet
iptables -A INPUT -p tcp -dport 23 -j DROP

adding a rule on a particular line number so that it can be matched accordingly

iptables -I INPUT 1 -p tcp --dport telnet -j DROP  this will insert the rule at 						   1 place in INPUT chain

Deleting and replacing rules

iptables -D INPUT 3	this will delete the rule on 3

iptables -R INPUT 2 -p tcp --dport 23 -j ACCEPT	this will replace the 2 rule and 						replace from the given rule.
Flushing the rules:-

iptables -F		flushes rules for all the chains

iptables -Z INPUT	this will zero a given chain

Creating a user defined tables

packet processing in iptabels happens from top to down

iptables -N INTRANET		this will create a new chain as intranet

iptables -A INPUT 1 -s 192.168.122.0/24 -j INTRANET

any traffic belonging to 192.168.122.0 subnet it will be sent to intranet chain
we can carry on further processing from there on

user defined chains must have unique names

example rule in user made chain

iptables -A INTRANET -p tcp --dport 23 -j DROP

iptables -E INTRANET EXTRANET 	renames the userdefined chain

Defining default chain policy

iptables -P INPUT DROP 		default policy is set to drop

Iptables  Matching

-s;	-src ;	--source;	-->for matching any source
-d;	-dst ;	--destinition;	-->for matching any dest

iptables -A INPUT -i eth1 -s 192.168.122.0/24 -j DROP

Thte above rule will block any traffic  from 192.168.122.0/24 coming from eth1 interface.

TCp works on layer 4 (transport layer) Connection oriented
UDP works on layer 5 (transport layer) Connection less

Udp based application:- TFTP(booting system/updating infrastructer)
			Syslog
			NTP
			DHCP binds on udp 67 and udp 68

ICMP--- Internet control messaging protocol

What happens when a system ping a remote machine.
ICMP TYPES:-
	a)echo-request	= ping
	b)echo-reply	= pong

PING - local system sends a "echo-request" via its OUTPUT chain.
Remote machine receives the "echo-request" on its INPUT chain.
Remote machine then respond with a "echo-reply" which the enters our INPUT chain.

If i drop echo-reply in INPUT chain then i won't be able to ping to anyone.
If i drop echo-request in INPUT chain then nobody will be able to ping me.


-p icmp  --icmp-type name/number 
iptables -p icmp --help  this show the list of option that can be given with iptables using icmp

To avoid writing of too many rules we can make use of multiport option in iptables 

iptables -A INPUT -p tcp -m multiport --destination-port 22,8080 -j DROP

This wil match both the port 22,8080 in one signle rule. For this kind of rule both the port should belong to same categorym tcp,tcp will work tcp,udp won't work

Matching on the basis of mac address

iptables -A INPUT -p tcp -m mac --mac-source lskdjfls -j drop

iptables is considered as a statefull firewall becoz its able to track the connection that flows thtough the firewall. so matche can be perform based on the states of varios services passing through the firewall.

IPtables states:- NEw,ESTABLISHED,RELATED,INVALID

iptables -A INPUT -p tcp --dport 22 -m state --state NEW -j DROP	This will drop any new ssh connection but old ssh session for this machine will be there.

iptables -A INPUT -p tcp --src 192.168.122.99 --dport 22 -m state --state NEW -j DROP 	This rule will block new connection form 192.168.122.99, but present connection will 												persistent#

IPtables LOGGING

iptables -A INPUT -p tcp --dport 22 -j LOG 		This will log all the said traffic for port 22

log for packets coming for you system the log in INPUT
log for packets routed thorugh your system then log in Forward chain

route add -net 192.168.5.0 netmask 255.255.255.0 gw 192.168.122.1

creating a linux machine into a router 
echo 3 > /proc/sys/net/ipv4/ip_forward


iptables -R FORWARD 2 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A FORWARD -s 192.168.122.0/24 -d 192.168.5.20 -j ACCEPT

#####################
Added in my workstation 	route add -net 192.168.5.0 netmask 255.255.255.0 gw 192.168.122.22

iptables -A FORWARD -p all -s 192.168.122.0/24 -d 192.168.122.20 -j ACCEPT	This will allow traffic form 122.0 to 5.20 
iptables -A FORWARD -p all -d 192.168.122.0/24 -s 192.168.5.20 -j ACCEPT -----> To allow reverse traffic from 192.168.5.20 to 192.168.122.0/24 network
#####################

rsync -avihP --progress -e "ssh -p 1422"  128.199.162.222:/home/bhavesh/Linuxacademy linuxacademy

######################

Network Address Transalation:-

Taking ip addresss of one subnet and you mask them to some other subnet that is consider to be masquerading.

Nat table:-
Nat table contains 3 chains

Prerouting:- Packet that are destined to the system that is accessible to the router
		rules written in this chain DNAT

Postrouting:- Masquerading and source nating

Output: - Locally source packet


Masquerade:-

iptables -t nat -A POSTROUTING -j MASQUERADE -s 192.168.5.0/24 -d 192.168.122.0/24

http://www.slashroot.in/linux-nat-network-address-translation-router-explained


###############################Learning AWS 5-10-2-15##############################

VPC, EC2, ELB, Route53, RDS, S3, ElasticCache


Amazon storage basics:-

1) AMaozon s3 its  a scalable files/obkect storage. these objects can be movie files,pdfs,images,mp3s. Objects can be from 1 byte ot 5tb, And you can store unlimited amount of data.
you can have multi threaded uploads from different source to your same aws account. you can have servers side encryption. you can server static content for your webpage. and it will server it in high available environment. Has a life cycle policy which allows to assign a policy to a bucket, you can integrate it with glacier. we can also use versioning

2) GLacier:-
	1) Glacier is an archiving storage type you use it for storing infrequently accessd		data, becoz it takes 2-6 hours to actually retrive data from glacier. 				only cost 1cent a gigabyte.
		You can integrate a policy in s3 which automaticly syncs you backups to 		glacier

3) EBS:-1)Its a block storage device you can write filesystem on it,
	2)EBS is redundant but its only redundant to your availibility zone.
	3)Need to customize it for creating a highly available architecture out of ebs.
	
* NOTE

S3 when you add an object it add the object within all the availibility zones. so your data is highly available and highly redudant within the region. Is a very much region specific and not availiblity zone specific.

Ephemeral storage:-
	1) this is temporary storage data, its lost as soon as the machine is shutdown.


Amazon security:-	AWS works on shared responsibility environment it means you as user			   are responsible for certain aspect of the security, and certain par			      is to be taken care by amazon.

User Responsibility:- 1) user management and access
			IAM (Identity Access Management)
			Multifactor Authentication
			Password/key Rotation
			Trusted Advisor
			Security Froups
			Access Control Lists
			VPC

AWS Responsibilty:- 1)Host physical server level and below
 		    2)Physical Enivronment Secutiy/Protection: Fire/power/climate managemet
		    3)Storage Device Decommissionin
		    4)Network Devie Security and acls
		    5)API access end points terminated with SSL for secure communication
		    6)DDOS Protection		
		    7)EC2 Instances cannot send spoofed data. aws does ingress and egress for a traffic genrating from ec2, they make sure the source is correct
		    8)Port scanning is against the law in aws.
	            9)Hypervisor isolation so instances on same physical host cannot talk to each other any how.


AWS ESSential gloabl infrastructer:- 

It consists of AWS Regions availablility zone and edge locations

#############

AMAZON S3

#############

ITS a highly availables by design, its an object storage it highly available coz it syncs your data across different availablity zone with in the region. which means if one availiblity zone becomes unavailable still your data is available.

Its design for 11 nines durablilty  and 99.99% availablity.
	Durablity means you r object is going to be there always
	availablity is self explained.
Cost:- cost per gig basis depends on your region, 12 cents a gig, also data out transfer is also charged. Data transfer within the same region is not chargeable. 

You also have a RRS option in S3, which is you dont 11 nines durability where the availabilty remains the samem here you can store the data which is easily reproducable.

Life cycle policies and object versioningL-
	you can define a life cycle policy where in you mention the object after storing for certain amount of time automatically archive it to glacier storage or you can have it deleted.

You can also enable versioning, you can have unlimited version of your objects.

Once versoning has been enable on s3 bucket you can't disable it but you can suspend it.

#############
EC2 Instance types
1)Ondemand instances:- these type of instances are created on demand say you want to test 3 instances of your application and you want to pay for just per hour than you can make use of on demand instance. high in price

2)Spot instances:- AWS allow to bid for unused ec2 instances. but aws does'nt guarantee the avaialiblity of the instances. its realyy  cheap

3)Reserverd Instances:-It guarantee you certain amt of processing power always available to you. eg - an application of your which demands certian compute 24/7 ,you know abt it and you want to provision something for the same, what you do is you purchase a reserve instance by paying a upfront amount which considerably lower than you ec2 ondemand instances. this reduces the cost by 30-40%
 3types of reserve instances:- Heavy,medium and light

#############
AWS IAM:-

Best practice is to create a administrator account after aws account is created

################################## HAPROXY 7-10-2015 ##################################

Differences Between Layer 4 and Layer 7 Load Balancing


Layer 4 load balancing operates at the intermediate transport layer, which deals with delivery of messages with no regard to the content of the messages. Transmission Control Protocol (TCP) is the Layer 4 protocol for Hypertext Transfer Protocol (HTTP) traffic on the Internet. Layer 4 load balancers simply forward network packets to and from the upstream server without inspecting the content of the packets. They can make limited routing decisions by inspecting the first few packets in the TCP stream.

Layer 7 load balancing operates at the high-level application layer, which deals with the actual content of each message. HTTP is the predominant Layer 7 protocol for website traffic on the Internet. Layer 7 load balancers route network traffic in a much more sophisticated way than Layer 4 load balancers, particularly applicable to TCP-based traffic such as HTTP. A Layer 7 load balancer terminates the network traffic and reads the message within. It can make a load-balancing decision based on the content of the message (the URL or cookie, for example). It then makes a new TCP connection to the selected upstream server (or reuses an existing one, by means of HTTP keepalives) and writes the request to the server.
Benefits of Layer 7 Load Balancing

Layer 7 load balancing is more CPU-intensive than packet-based Layer 4 load balancing, but rarely causes degraded performance on a modern server. Layer 7 load balancing enables the load balancer to make smarter load-balancing decisions, and to apply optimizations and changes to the content (such as compression and encryption). It uses buffering to offload slow connections from the upstream servers, which improves performance.

A device that performs Layer 7 load balancing is often referred to as a reverse-proxy server.
An Example of Layer 7 Load Balancing

Let’s look at a simple example. A user visits a high-traffic website. Over the course of the user’s session, he or she might request static content such as images or video, dynamic content such as a news feed, and even transactional information such as order status. Layer 7 load balancing allows the load balancer to route a request based on information in the request itself, such as what kind of content is being requested. So now a request for an image or video can be routed to the servers that store it and are highly optimized to serve up multimedia content. Requests for transactional information such as a discounted price can be routed to the application server responsible for managing pricing. With Layer 7 load balancing, network and application architects can create a highly tuned and optimized server infrastructure or application delivery network that is both reliable and efficiently scales to meet demand.



HAProxy Configuration

HAProxy's configuration file is divided into two major sections:

    Global: sets process-wide parameters
    Proxies: consists of defaults, listen, frontend, and backend parameters



#################################### Learning DATABASE###################################

RDBMS

RDBMS stands for Relational Database Management System.

RDBMS is the basis for SQL, and for all modern database systems such as MS SQL Server, IBM DB2, Oracle, MySQL, and Microsoft Access.

The data in RDBMS is stored in database objects called tables.

A table is a collection of related data entries and it consists of columns and rows.


Some of The Most Important SQL Commands

    SELECT - extracts data from a database
    UPDATE - updates data in a database
    DELETE - deletes data from a database
    INSERT INTO - inserts new data into a database
    CREATE DATABASE - creates a new database
    ALTER DATABASE - modifies a database
    CREATE TABLE - creates a new table
    ALTER TABLE - modifies a table
    DROP TABLE - deletes a table
    CREATE INDEX - creates an index (search key)
    DROP INDEX - deletes an index


*The SELECT DISTINCT statement is used to return only distinct (different) values.

*The WHERE clause is used to extract only those records that fulfill a specified criterion.


#Text Fields vs. Numeric Fields

SQL requires single quotes around text values (most database systems will also allow double quotes).

However, numeric fields should not be enclosed in quotes:

##
The SQL AND & OR Operators

The AND operator displays a record if both the first condition AND the second condition are true.

The OR operator displays a record if either the first condition OR the second condition is true.

The SQL ORDER BY Keyword

The ORDER BY keyword is used to sort the result-set by one or more columns.

The ORDER BY keyword sorts the records in ascending order by default. To sort the records in a descending order, you can use the DESC keyword.

###################################################### Raid ######################

mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sda6 /dev/sda7

mdadm --detail /dev/md0

mkfs.ext4 /dev/md0

incase of issue mdo will be booted like md127

mdadm --stop /dev/md127

mdadm --assemble --scan

#####
mdadm /dev/md0 --fail /dev/sdb1 #Marks /dev/sdb1 as faulty
 mdadm --remove /dev/md0 /dev/sdb1 #Removes /dev/sdb1 from the array 

Afterwards, when you have a new drive for replacement, re-add the drive again:
mdadm /dev/md0 --add /dev/sdb1 

Note that the steps detailed above apply for systems with hot-swappable disks. If you do not have such technology, you will also have to stop a current array, and shutdown your system first in order to replace the part:

 mdadm --stop /dev/md0
# shutdown -h now

Then add the new drive and re-assemble the array:
# mdadm /dev/md0 --add /dev/sdb1
mdadm --grow --raid-devices=4 /dev/md1

# mdadm --assemble /dev/md0 /dev/sdb1 /dev/sdc1 
